
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="robots" content="" />

  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro|Source+Sans+Pro:300,400,400i,700" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="https://ihoromi4.github.io/theme/stylesheet/style.min.css">

  <link rel="stylesheet" type="text/css" href="https://ihoromi4.github.io/theme/pygments/github.min.css">
  <link rel="stylesheet" type="text/css" href="https://ihoromi4.github.io/theme/font-awesome/css/font-awesome.min.css">


    <link href="https://ihoromi4.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Ihor Omelchenko Blog Atom">





<meta name="author" content="Ihor Omelchenko" />
<meta name="description" content="Навчання з підкріпленням - це напрямок штучного інтелекту, який спрямований на розвиток алгоритмів та моделей, здатних самостійно вчитися та приймати рішення на основі взаємодії з навколишнім середовищем. Основна ідея полягає в тому, щоб створити агентів, які можуть взаємодіяти з навколишнім світом, отримувати відгуки у вигляді підкріплення або покарання і вдосконалювати свої …" />
<meta name="keywords" content="AI, ML, RL">

<meta property="og:site_name" content="Ihor Omelchenko Blog"/>
<meta property="og:title" content="Про навчання з підкріпленням"/>
<meta property="og:description" content="Навчання з підкріпленням - це напрямок штучного інтелекту, який спрямований на розвиток алгоритмів та моделей, здатних самостійно вчитися та приймати рішення на основі взаємодії з навколишнім середовищем. Основна ідея полягає в тому, щоб створити агентів, які можуть взаємодіяти з навколишнім світом, отримувати відгуки у вигляді підкріплення або покарання і вдосконалювати свої …"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="https://ihoromi4.github.io/pro-navchannia-z-pidkriplenniam-ua.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2023-09-02 14:00:00+03:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="https://ihoromi4.github.io/author/ihor-omelchenko.html">
<meta property="article:section" content="Reinforcement Learning"/>
<meta property="article:tag" content="AI"/>
<meta property="article:tag" content="ML"/>
<meta property="article:tag" content="RL"/>
<meta property="og:image" content="">

  <title>Ihor Omelchenko Blog &ndash; Про навчання з підкріпленням</title>

</head>
<body>



  <aside>
    <div>
      <a href="https://ihoromi4.github.io">
        <img src="https://ihoromi4.github.io/theme/img/profile.png" alt="Ihor Omelchenko" title="Ihor Omelchenko">
      </a>
      <h1><a href="https://ihoromi4.github.io">Ihor Omelchenko</a></h1>

<p>Python | C/C++ | Data Scientist | ML Developer</p>
      <nav>
        <ul class="list">
          <li><a href="https://ihoromi4.github.io/pages/about.html#about">About</a></li>

        </ul>
      </nav>

      <ul class="social">
        <li><a class="sc-github" href="https://github.com/ihoromi4" target="_blank"><i class="fa fa-github"></i></a></li>
        <li><a class="sc-linkedin" href="https://www.linkedin.com/in/ihor-omelchenko-38839b12a" target="_blank"><i class="fa fa-linkedin"></i></a></li>
        <li><a class="sc-facebook" href="https://www.facebook.com/profile.php?id=100004291211446" target="_blank"><i class="fa fa-facebook"></i></a></li>
        <li><a class="sc-telegram" href="https://t.me/ihoromi4" target="_blank"><i class="fa fa-telegram"></i></a></li>
      </ul>
    </div>


  </aside>
  <main>


<article class="single">
  <header>
      
    <h1 id="pro-navchannia-z-pidkriplenniam">Про навчання з підкріпленням</h1>
    <p>
          Posted on сб 02 вересня 2023 in <a href="https://ihoromi4.github.io/category/reinforcement-learning.html">Reinforcement Learning</a>


    </p>
  </header>


  <div>
    <p>Навчання з підкріпленням - це напрямок штучного інтелекту, який спрямований на розвиток алгоритмів та моделей, здатних самостійно вчитися та приймати рішення на основі взаємодії з навколишнім середовищем. </p>
<p>Основна ідея полягає в тому, щоб створити агентів, які можуть взаємодіяти з навколишнім світом, отримувати відгуки у вигляді підкріплення або покарання і вдосконалювати свої дії, максимізуючи нагороду. Навчання з підкріпленням є ключовим компонентом для створення інтелігентних систем, які можуть досягати високих результатів у завданнях без заздалегідь заданого алгоритму.</p>
<h1>Три види навчання</h1>
<p>В момент написання цієї статті домінуючим підходом у штучному інтелекті є методи, що вчаться. Їх окреми яскравий представник - глибокі нейронні мережі. Такі алгоритми здатні знаходити алгоритм за набором прикладів.</p>
<p>Перша нейромережева модель природнього нейрону з'явилася у далекому 1942 році[1].  </p>
<p>Традиційно методи машинного навчання поділяють на три великі групи:</p>
<ul>
<li>Навчання з учителем (англ. Supervised Learning, SL)</li>
<li>Навчання без вчителя (англ. Unsupervised Learning, UL)</li>
<li>Навчання з підкріпленням (англ. Reinforcement Learning, RL)</li>
</ul>
<p>Навчання з учителем об'єднує підходи основна ціль яких - узагальнення навчальних даних. </p>
<p>Методи "навчання без вчителя" вирішують проблему стискання даних шляхом знаходження внутрішньої структури.</p>
<p>Навчання з підкріпленням вивчає як агенту навчитися діяти у середовищі оптимально.</p>
<h1>Коротка історія навчання з підкріпленням</h1>
<p>Навчання з підкріпленням виникло з двох паралельних віток досліджень[2], які розвивалися майже незалежно.</p>
<p>Перша виникла з досліджень поведінки тварин та вивчала навчання шляхом спроб та помилок.</p>
<p>Друга вітка походила з теорії оптимального управління. Термін "оптимальне управління" почав вживатися у 1950-х. Цей підхід працював з функціями цінності, динамічним програмуванням та, більшою мірою, не допускав навчання.</p>
<p>Також можна виділити третій підхід, заснований на часових різницях (англ. temporal-difference).</p>
<p>Сучасне навчання з підкіпленням виникло в 1980-х шляхом об'єднаня трьох вище згаданих віток в одну.</p>
<h1>Особливості навчання з підкріпленням</h1>
<p>На відміну від інших підходів, навчання з підкріпленням працює з послідовним прийняттям рішень, не потребує заздалегіть відомої моделі середовища.
Навчання з підкріпленням породжує проблеми які не можна знайти у навчанні з вчителем чи навчанні без вчителя. Коли ми починаємо вирішувати задачу навчання з підкріпленням, ми не маємо готового датасету для навчання, натомість агент збирає дані в процесі взаємодії з середовищем. Агент сам збирає датасет для навчання.</p>
<h1>Формалізм навчання з підкріпленням</h1>
<p>Головним формалізмом в навчанні з підкріпленням є <a href="https://en.wikipedia.org/wiki/Markov_decision_process">Марківський процес прийняття рішень</a>. Це загальний спосіб описати стохастичний процес з дискретним часом у якому дії агента впливають на динаміку середовища.</p>
<p>Головні складові навчання з підкріпленням:</p>
<ul>
<li>Середовище (англ. Environment): все, окрім агента</li>
<li>Агент (англ. Agent): сутність, що здатна спостерігати середовище, та обирати дії.</li>
</ul>
<p>Для кожної системи вибір межі розділу між середовищем та агентом дещо довільний. Наприклад, у ситуації водій-автомобіль-шосе, можливо декілька представлень. Перший варіант: водій це агент, який управляє автомобілем. Інший - автомобіль з водієм це агент, який їде по шосе.</p>
<p>Основні поняття MDP:</p>
<ul>
<li>Стан <span class="math">\(s_t\)</span>: вкожен момент часу <span class="math">\(t\)</span> середовище перебуває у певному стані <span class="math">\(s_t\)</span>. Конкретне представлення стану зележить від середовища. Зазвичай це вектор, або матриця.</li>
<li>Дія <span class="math">\(a_t\)</span>: в кожен момент часу <span class="math">\(t\)</span> агент може обрати одну можливу дію з набору, який визначає середовище.</li>
<li>Нагорода <span class="math">\(r_t\)</span>: після виконання дії агент отримує від середовища нагороду. Особливість навчання з підкіпленням в тому, що ця нагорода не обов'язково пов'язана з попередньою дією, вона може бути наслідком будь-якої дії що була обрана в минулому.</li>
<li>Функція нагороди <span class="math">\(R(s_t, s_{t+1})\)</span>: функція, яка визначає яку саме чисельну нагороду отримає агент перейшовши з стану <span class="math">\(s_t\)</span> до стану <span class="math">\(s_{t+1}\)</span>.</li>
<li>Функція переходу <span class="math">\(P(s_t, a_t, s_{t+1})\)</span>: це розподіл імовірностей переходів між станами при обраних діях.</li>
</ul>
<p>Формально цей формалізм застосовний лише для опису систем, для яких виконується марківське припущення, іншими словами середовище має <a href="https://uk.wikipedia.org/wiki/%D0%9C%D0%B0%D1%80%D0%BA%D0%BE%D0%B2%D1%81%D1%8C%D0%BA%D0%B0_%D0%B2%D0%BB%D0%B0%D1%81%D1%82%D0%B8%D0%B2%D1%96%D1%81%D1%82%D1%8C">марковську властивість</a>. Це можна пояснити як відсутність пам'яті у стохастичному процесі. Іншими словами, все що потрібно знати агенту для оптимальної поведінки міститься в останному стані середовища <span class="math">\(s_t\)</span>, а всі попередні стани не потрібні. Формально це можна записати як незалежність умовного статистичного розподілу від попередніх станів, кріп останнього:
</p>
<div class="math">$$P(s_{t+1}| s_{t}) = P(s_{t+1} | s_t, s_{t-1}, ..., s_2, s_1)$$</div>
<h1>Джерела</h1>
<ol>
<li>McCulloch, W. S., &amp; Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity. The bulletin of mathematical biophysics, 5, 115-133.</li>
<li>Sutton, R. S., &amp; Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.</li>
</ol>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','boldsymbol.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="https://ihoromi4.github.io/tag/ai.html">AI</a>
      <a href="https://ihoromi4.github.io/tag/ml.html">ML</a>
      <a href="https://ihoromi4.github.io/tag/rl.html">RL</a>
    </p>
  </div>





</article>

    <footer>
<p>&copy;  </p>
<p>    Powered by <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a>
</p>    </footer>
  </main>




<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Ihor Omelchenko Blog ",
  "url" : "https://ihoromi4.github.io",
  "image": "",
  "description": "Description"
}
</script>

</body>
</html>