Title: Про навчання з підкріпленням
Date: 2023-09-02 14:00
Lang: ua
Autor: Ihor Omelchenko
Category: Reinforcement Learning
Tags: AI, ML, RL
Status: published


Навчання з підкріпленням - це напрямок штучного інтелекту, який спрямований на розвиток алгоритмів та моделей, здатних самостійно вчитися та приймати рішення на основі взаємодії з навколишнім середовищем. 

Основна ідея полягає в тому, щоб створити агентів, які можуть взаємодіяти з навколишнім світом, отримувати відгуки у вигляді підкріплення або покарання і вдосконалювати свої дії, максимізуючи нагороду. Навчання з підкріпленням є ключовим компонентом для створення інтелігентних систем, які можуть досягати високих результатів у завданнях без заздалегідь заданого алгоритму.

# Три види навчання
В момент написання цієї статті домінуючим підходом у штучному інтелекті є методи, що вчаться. Їх окреми яскравий представник - глибокі нейронні мережі. Такі алгоритми здатні знаходити алгоритм за набором прикладів.

Перша нейромережева модель природнього нейрону з'явилася у далекому 1942 році[1].  

Традиційно методи машинного навчання поділяють на три великі групи:

* Навчання з учителем (англ. Supervised Learning, SL)
* Навчання без вчителя (англ. Unsupervised Learning, UL)
* Навчання з підкріпленням (англ. Reinforcement Learning, RL)

Навчання з учителем об'єднує підходи основна ціль яких - узагальнення навчальних даних. 

Методи "навчання без вчителя" вирішують проблему стискання даних шляхом знаходження внутрішньої структури.

Навчання з підкріпленням вивчає як агенту навчитися діяти у середовищі оптимально.

# Коротка історія навчання з підкріпленням
Навчання з підкріпленням виникло з двох паралельних віток досліджень[2], які розвивалися майже незалежно.

Перша виникла з досліджень поведінки тварин та вивчала навчання шляхом спроб та помилок.

Друга вітка походила з теорії оптимального управління. Термін "оптимальне управління" почав вживатися у 1950-х. Цей підхід працював з функціями цінності, динамічним програмуванням та, більшою мірою, не допускав навчання.

Також можна виділити третій підхід, заснований на часових різницях (англ. temporal-difference).

Сучасне навчання з підкіпленням виникло в 1980-х шляхом об'єднаня трьох вище згаданих віток в одну.

# Особливості навчання з підкріпленням
На відміну від інших підходів, навчання з підкріпленням працює з послідовним прийняттям рішень, не потребує заздалегіть відомої моделі середовища.
Навчання з підкріпленням породжує проблеми які не можна знайти у навчанні з вчителем чи навчанні без вчителя. Коли ми починаємо вирішувати задачу навчання з підкріпленням, ми не маємо готового датасету для навчання, натомість агент збирає дані в процесі взаємодії з середовищем. Агент сам збирає датасет для навчання.

# Формалізм навчання з підкріпленням
Головним формалізмом в навчанні з підкріпленням є [Марківський процес прийняття рішень](https://en.wikipedia.org/wiki/Markov_decision_process). Це загальний спосіб описати стохастичний процес з дискретним часом у якому дії агента впливають на динаміку середовища.

Головні складові навчання з підкріпленням:

* Середовище (англ. Environment): все, окрім агента
* Агент (англ. Agent): сутність, що здатна спостерігати середовище, та обирати дії.

Для кожної системи вибір межі розділу між середовищем та агентом дещо довільний. Наприклад, у ситуації водій-автомобіль-шосе, можливо декілька представлень. Перший варіант: водій це агент, який управляє автомобілем. Інший - автомобіль з водієм це агент, який їде по шосе.

Основні поняття MDP:

* Стан $s_t$: вкожен момент часу $t$ середовище перебуває у певному стані $s_t$. Конкретне представлення стану зележить від середовища. Зазвичай це вектор, або матриця.
* Дія $a_t$: в кожен момент часу $t$ агент може обрати одну можливу дію з набору, який визначає середовище.
* Нагорода $r_t$: після виконання дії агент отримує від середовища нагороду. Особливість навчання з підкіпленням в тому, що ця нагорода не обов'язково пов'язана з попередньою дією, вона може бути наслідком будь-якої дії що була обрана в минулому.
* Функція нагороди $R(s_t, s_{t+1})$: функція, яка визначає яку саме чисельну нагороду отримає агент перейшовши з стану $s_t$ до стану $s_{t+1}$.
* Функція переходу $P(s_t, a_t, s_{t+1})$: це розподіл імовірностей переходів між станами при обраних діях.

Формально цей формалізм застосовний лише для опису систем, для яких виконується марківське припущення, іншими словами середовище має [марковську властивість](https://uk.wikipedia.org/wiki/%D0%9C%D0%B0%D1%80%D0%BA%D0%BE%D0%B2%D1%81%D1%8C%D0%BA%D0%B0_%D0%B2%D0%BB%D0%B0%D1%81%D1%82%D0%B8%D0%B2%D1%96%D1%81%D1%82%D1%8C). Це можна пояснити як відсутність пам'яті у стохастичному процесі. Іншими словами, все що потрібно знати агенту для оптимальної поведінки міститься в останному стані середовища $s_t$, а всі попередні стани не потрібні. Формально це можна записати як незалежність умовного статистичного розподілу від попередніх станів, кріп останнього:
$$P(s_{t+1}| s_{t}) = P(s_{t+1} | s_t, s_{t-1}, ..., s_2, s_1)$$

# Джерела
1. McCulloch, W. S., & Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity. The bulletin of mathematical biophysics, 5, 115-133.
2. Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.

